{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8231faa0-4fae-4cca-8bc7-ed5b8912253e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data (first 5 rows):\n",
      "               name  age  salary  experience\n",
      "0       Alice Smith   28   75000           5\n",
      "1       Bob Johnson   34   92000          10\n",
      "2  Charlie Williams   41  105000          15\n",
      "3       David Brown   23   32000           2\n",
      "4         Eva Jones   30   85000           7\n",
      "\n",
      "1. Min-Max Normalization:\n",
      "               name       age    salary  experience\n",
      "0       Alice Smith  0.135135  0.380531    0.090909\n",
      "1       Bob Johnson  0.297297  0.530973    0.242424\n",
      "2  Charlie Williams  0.486486  0.646018    0.393939\n",
      "3       David Brown  0.000000  0.000000    0.000000\n",
      "4         Eva Jones  0.189189  0.469027    0.151515\n",
      "\n",
      "2. Z-score Normalization (Standardization):\n",
      "               name       age    salary  experience\n",
      "0       Alice Smith -1.114831 -0.461168   -1.072113\n",
      "1       Bob Johnson -0.416607  0.271529   -0.402042\n",
      "2  Charlie Williams  0.397988  0.831827    0.268028\n",
      "3       David Brown -1.696685 -2.314462   -1.474155\n",
      "4         Eva Jones -0.882090 -0.030170   -0.804084\n",
      "\n",
      "3. Decimal Scaling Normalization:\n",
      "               name   age  salary  experience\n",
      "0       Alice Smith  0.28   0.075        0.05\n",
      "1       Bob Johnson  0.34   0.092        0.10\n",
      "2  Charlie Williams  0.41   0.105        0.15\n",
      "3       David Brown  0.23   0.032        0.02\n",
      "4         Eva Jones  0.30   0.085        0.07\n",
      "\n",
      "4. L2 Normalization (row-wise):\n",
      "               name       age  salary  experience\n",
      "0       Alice Smith  0.000373     1.0    0.000067\n",
      "1       Bob Johnson  0.000370     1.0    0.000109\n",
      "2  Charlie Williams  0.000390     1.0    0.000143\n",
      "3       David Brown  0.000719     1.0    0.000062\n",
      "4         Eva Jones  0.000353     1.0    0.000082\n",
      "\n",
      "5. L1 Normalization (row-wise):\n",
      "               name       age    salary  experience\n",
      "0       Alice Smith  0.000373  0.999560    0.000067\n",
      "1       Bob Johnson  0.000369  0.999522    0.000109\n",
      "2  Charlie Williams  0.000390  0.999467    0.000143\n",
      "3       David Brown  0.000718  0.999219    0.000062\n",
      "4         Eva Jones  0.000353  0.999565    0.000082\n",
      "\n",
      "6. Robust Scaling:\n",
      "               name       age    salary  experience\n",
      "0       Alice Smith -0.705882 -0.407767   -0.736842\n",
      "1       Bob Johnson -0.235294  0.252427   -0.210526\n",
      "2  Charlie Williams  0.313725  0.757282    0.315789\n",
      "3       David Brown -1.098039 -2.077670   -1.052632\n",
      "4         Eva Jones -0.549020 -0.019417   -0.526316\n",
      "\n",
      "7. Max-Abs Scaling:\n",
      "               name       age    salary  experience\n",
      "0       Alice Smith  0.466667  0.517241    0.142857\n",
      "1       Bob Johnson  0.566667  0.634483    0.285714\n",
      "2  Charlie Williams  0.683333  0.724138    0.428571\n",
      "3       David Brown  0.383333  0.220690    0.057143\n",
      "4         Eva Jones  0.500000  0.586207    0.200000\n",
      "\n",
      "8. Log Transformation (using log1p):\n",
      "               name       age     salary  experience\n",
      "0       Alice Smith  3.367296  11.225257    1.791759\n",
      "1       Bob Johnson  3.555348  11.429555    2.397895\n",
      "2  Charlie Williams  3.737670  11.561725    2.772589\n",
      "3       David Brown  3.178054  10.373522    1.098612\n",
      "4         Eva Jones  3.433987  11.350418    2.079442\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, MaxAbsScaler, Normalizer\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('data.csv')\n",
    "print(\"Original Data (first 5 rows):\")\n",
    "print(df.head())\n",
    "\n",
    "# Define the numerical columns to be normalized\n",
    "cols = ['age', 'salary', 'experience']\n",
    "\n",
    "# 1. Min-Max Normalization\n",
    "scaler_minmax = MinMaxScaler()\n",
    "df_minmax = df.copy()\n",
    "df_minmax[cols] = scaler_minmax.fit_transform(df[cols])\n",
    "print(\"\\n1. Min-Max Normalization:\")\n",
    "print(df_minmax.head())\n",
    "\n",
    "# 2. Z-score Normalization (Standardization)\n",
    "scaler_standard = StandardScaler()\n",
    "df_standard = df.copy()\n",
    "df_standard[cols] = scaler_standard.fit_transform(df[cols])\n",
    "print(\"\\n2. Z-score Normalization (Standardization):\")\n",
    "print(df_standard.head())\n",
    "\n",
    "# 3. Decimal Scaling Normalization (Custom Implementation)\n",
    "def decimal_scaling(series):\n",
    "    max_abs = series.abs().max()\n",
    "    # Calculate the scaling factor as the next power of 10 greater than max_abs\n",
    "    power = np.ceil(np.log10(max_abs + 1))\n",
    "    return series / (10**power)\n",
    "\n",
    "df_decimal = df.copy()\n",
    "df_decimal[cols] = df_decimal[cols].apply(decimal_scaling)\n",
    "print(\"\\n3. Decimal Scaling Normalization:\")\n",
    "print(df_decimal.head())\n",
    "\n",
    "# 4. L2 Normalization (row-wise)\n",
    "normalizer_l2 = Normalizer(norm='l2')\n",
    "df_l2 = df.copy()\n",
    "df_l2_values = normalizer_l2.fit_transform(df[cols])\n",
    "df_l2[cols] = df_l2_values\n",
    "print(\"\\n4. L2 Normalization (row-wise):\")\n",
    "print(df_l2.head())\n",
    "\n",
    "# 5. L1 Normalization (row-wise)\n",
    "normalizer_l1 = Normalizer(norm='l1')\n",
    "df_l1 = df.copy()\n",
    "df_l1_values = normalizer_l1.fit_transform(df[cols])\n",
    "df_l1[cols] = df_l1_values\n",
    "print(\"\\n5. L1 Normalization (row-wise):\")\n",
    "print(df_l1.head())\n",
    "\n",
    "# 6. Robust Scaling (using median and IQR)\n",
    "scaler_robust = RobustScaler()\n",
    "df_robust = df.copy()\n",
    "df_robust[cols] = scaler_robust.fit_transform(df[cols])\n",
    "print(\"\\n6. Robust Scaling:\")\n",
    "print(df_robust.head())\n",
    "\n",
    "# 7. Max-Abs Scaling\n",
    "scaler_maxabs = MaxAbsScaler()\n",
    "df_maxabs = df.copy()\n",
    "df_maxabs[cols] = scaler_maxabs.fit_transform(df[cols])\n",
    "print(\"\\n7. Max-Abs Scaling:\")\n",
    "print(df_maxabs.head())\n",
    "\n",
    "# 8. Log Transformation (using np.log1p to handle zeros)\n",
    "df_log = df.copy()\n",
    "df_log[cols] = df_log[cols].apply(lambda x: np.log1p(x))\n",
    "print(\"\\n8. Log Transformation (using log1p):\")\n",
    "print(df_log.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9599c169-5069-4722-a6fe-785a38db5309",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
